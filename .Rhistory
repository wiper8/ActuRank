groups_to_join <- which(sapply(lapply(clusters, `[[`, "names"), function(noms) any(unique_pairs[[p]] %in% noms)))
joint_distr_from_clusters <- join_clusters(clusters, groups_to_join)
joint_distr_from_clusters$names <- names(joint_distr_from_clusters$domains)
if (isFALSE(all.equal(sum(joint_distr_from_clusters$joint_distr$p), 1))) {
print(d)
stop("Erreur de probs")
}
# update la distribution
joint_density <- update_scores_exact(
joint_distr_from_clusters,
scores=scores_subset[unique_pairs_game_i[[p]], , drop = FALSE],
dataset
)
# re-simplifier
joint_density <- simplifier_joint_dependancy(
joint_density,
seuil = 1 - max(0.9, min(0.999, (0.95 - 0.999)/(200000-1000) * (nrow(joint_density$joint_distr) - 1000) + 0.999)),
absolute_max_dim = 500000,
min_no_simplif = 200,
verbose = TRUE
)
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
clusters <- c(new_clusters, clusters[!seq_along(clusters) %in% groups_to_join])
print(paste0(length(clusters), " clusters, length ", paste0(sapply(clusters, function(x) length(x$names)), collapse = ", ")))
order(pairs_pseudo_credibl, decreasing = TRUE)
p
p=6
# ajouter les nouveaux joueurs de cette partie i
players_this_game <- unique(unlist(scores_subset[unique_pairs_game_i[[p]], c("joueur_A2", "joueur_B1"), drop = FALSE]))
players_this_game
for(n in players_this_game[!players_this_game %in% unlist(sapply(clusters, `[[`, "names"))]) {
print(paste0("Ajout de : ", n, collapse = ""))
clusters <- add_player_dependancy(n, clusters)
}
# créer la distribution conjointe nécessaire pour la paire
groups_to_join <- which(sapply(lapply(clusters, `[[`, "names"), function(noms) any(unique_pairs[[p]] %in% noms)))
joint_distr_from_clusters <- join_clusters(clusters, groups_to_join)
joint_distr_from_clusters$names <- names(joint_distr_from_clusters$domains)
if (isFALSE(all.equal(sum(joint_distr_from_clusters$joint_distr$p), 1))) {
print(d)
stop("Erreur de probs")
}
# update la distribution
joint_density <- update_scores_exact(
joint_distr_from_clusters,
scores=scores_subset[unique_pairs_game_i[[p]], , drop = FALSE],
dataset
)
# re-simplifier
joint_density <- simplifier_joint_dependancy(
joint_density,
seuil = 1 - max(0.9, min(0.999, (0.95 - 0.999)/(200000-1000) * (nrow(joint_density$joint_distr) - 1000) + 0.999)),
absolute_max_dim = 500000,
min_no_simplif = 200,
verbose = TRUE
)
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
clusters <- c(new_clusters, clusters[!seq_along(clusters) %in% groups_to_join])
print(paste0(length(clusters), " clusters, length ", paste0(sapply(clusters, function(x) length(x$names)), collapse = ", ")))
order(pairs_pseudo_credibl, decreasing = TRUE)
p
p=5
# ajouter les nouveaux joueurs de cette partie i
players_this_game <- unique(unlist(scores_subset[unique_pairs_game_i[[p]], c("joueur_A2", "joueur_B1"), drop = FALSE]))
players_this_game
for(n in players_this_game[!players_this_game %in% unlist(sapply(clusters, `[[`, "names"))]) {
print(paste0("Ajout de : ", n, collapse = ""))
clusters <- add_player_dependancy(n, clusters)
}
# créer la distribution conjointe nécessaire pour la paire
groups_to_join <- which(sapply(lapply(clusters, `[[`, "names"), function(noms) any(unique_pairs[[p]] %in% noms)))
joint_distr_from_clusters <- join_clusters(clusters, groups_to_join)
joint_distr_from_clusters$names <- names(joint_distr_from_clusters$domains)
if (isFALSE(all.equal(sum(joint_distr_from_clusters$joint_distr$p), 1))) {
print(d)
stop("Erreur de probs")
}
# update la distribution
joint_density <- update_scores_exact(
joint_distr_from_clusters,
scores=scores_subset[unique_pairs_game_i[[p]], , drop = FALSE],
dataset
)
# re-simplifier
joint_density <- simplifier_joint_dependancy(
joint_density,
seuil = 1 - max(0.9, min(0.999, (0.95 - 0.999)/(200000-1000) * (nrow(joint_density$joint_distr) - 1000) + 0.999)),
absolute_max_dim = 500000,
min_no_simplif = 200,
verbose = TRUE
)
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
clusters <- c(new_clusters, clusters[!seq_along(clusters) %in% groups_to_join])
print(paste0(length(clusters), " clusters, length ", paste0(sapply(clusters, function(x) length(x$names)), collapse = ", ")))
order(pairs_pseudo_credibl, decreasing = TRUE)
p
p=8
# ajouter les nouveaux joueurs de cette partie i
players_this_game <- unique(unlist(scores_subset[unique_pairs_game_i[[p]], c("joueur_A2", "joueur_B1"), drop = FALSE]))
players_this_game
for(n in players_this_game[!players_this_game %in% unlist(sapply(clusters, `[[`, "names"))]) {
print(paste0("Ajout de : ", n, collapse = ""))
clusters <- add_player_dependancy(n, clusters)
}
# créer la distribution conjointe nécessaire pour la paire
groups_to_join <- which(sapply(lapply(clusters, `[[`, "names"), function(noms) any(unique_pairs[[p]] %in% noms)))
joint_distr_from_clusters <- join_clusters(clusters, groups_to_join)
joint_distr_from_clusters$names <- names(joint_distr_from_clusters$domains)
if (isFALSE(all.equal(sum(joint_distr_from_clusters$joint_distr$p), 1))) {
print(d)
stop("Erreur de probs")
}
# update la distribution
joint_density <- update_scores_exact(
joint_distr_from_clusters,
scores=scores_subset[unique_pairs_game_i[[p]], , drop = FALSE],
dataset
)
# re-simplifier
joint_density <- simplifier_joint_dependancy(
joint_density,
seuil = 1 - max(0.9, min(0.999, (0.95 - 0.999)/(200000-1000) * (nrow(joint_density$joint_distr) - 1000) + 0.999)),
absolute_max_dim = 500000,
min_no_simplif = 200,
verbose = TRUE
)
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
max_cluster_size = Inf
min_cluster_size = 1
joint_distr_size_skip = 500
MI_thresh_for_indep <- 0.05
length(joint_density$domains) == 1 ||
nrow(joint_density$joint_distr) <= joint_distr_size_skip ||
MI_thresh_for_indep == 0
# faire un gros cluster et pour chq élément regarder si on peut les retirer d'une manière greedy
combins <- generate_partitions(length(joint_density$domains), 2, min_cluster_size = min_cluster_size)
combins
combins_len <- sapply(combins, length)
combins_len
table(combins_len)
length(combins)
MI <- rep(Inf, length(combins))
sort(unique(combins_len))
unique(combins_len)
combins_len
len_out = 1
for (j in which(combins_len == len_out)) {
MI[j] <- relative_mutual_information(joint_density, combins[[j]])
}
MI[which(combins_len == len_out)]
MI_thresh_for_indep
MI[which(combins_len == len_out)]
min(MI) < MI_thresh_for_indep
min(MI)
print(paste0(sum(MI < Inf), " / ", length(MI)))
# l'exclure du gros cluster in
# TODO moyen d'optimiser car dédouble les calculs déjà faits dans mutual_info()
new_clusters <- marginal_joint_dependancy(joint_density, combins[[which.min(MI)]], format = 2)
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
recluster_dependancy <- function(joint_density, dataset,
max_cluster_size = Inf, min_cluster_size = 1,
joint_distr_size_skip = 500) {
if (dataset == "ping") MI_thresh_for_indep <- 0.05
else MI_thresh_for_indep <- 0
if (
length(joint_density$domains) == 1 ||
nrow(joint_density$joint_distr) <= joint_distr_size_skip ||
MI_thresh_for_indep == 0
) {
joint_density$names <- names(joint_density$domains)
return(list(joint_density))
}
print("ici")
# faire un gros cluster et pour chq élément regarder si on peut les retirer d'une manière greedy
combins <- generate_partitions(length(joint_density$domains), 2, min_cluster_size = min_cluster_size)
combins_len <- sapply(combins, length)
if (length(combins) > 0) {
MI <- rep(Inf, length(combins))
# faire toutes les combins qu'on rejette 1 personne, ensuite toutes rejet 2pers, etc.
for (len_out in unique(combins_len)) {
for (j in which(combins_len == len_out)) {
MI[j] <- relative_mutual_information(joint_density, combins[[j]])
}
# si le min de MI de ce groupe est sous le seuil, on essaie d'augmenter la
# taille de rejetés (donc on continue à chercher plus de combins)
# pcq plus on rejette bcp, plus min(MI) sera élevé donc on veut arrêter
# quand tout le groupe dépasse le seuil
if (min(MI[which(combins_len == len_out)]) >= MI_thresh_for_indep) {
# print(paste0("MI : ", paste0(round(MI, 3), collapse = ", ")))
# on compare toujours un séparation entre 2 subsets, comme si on a 2 variables
# MI = H(X) + H(Y) - H(X,Y)
# max(H(X), H(Y)) <= H(X)+H(Y)-max(H(X), H(Y))
# borne sup(MI) = H(X) + H(Y) - max(H(x),H(Y))
if (min(MI) < MI_thresh_for_indep) {
print(paste0(sum(MI < Inf), " / ", length(MI)))
# l'exclure du gros cluster in
# TODO moyen d'optimiser car dédouble les calculs déjà faits dans mutual_info()
new_clusters <- marginal_joint_dependancy(joint_density, combins[[which.min(MI)]], format = 2)
return(
c(
new_clusters[2],
recluster_dependancy(new_clusters[[1]], dataset)
)
)
} else {
break
}
}
}
if (length(joint_density$domains) > max_cluster_size) {
MI <- MI[length(joint_density$domains) - sapply(combins, length) <= max_cluster_size]
combins <- combins[length(joint_density$domains) - sapply(combins, length) <= max_cluster_size]
new_clusters <- marginal_joint_dependancy(
joint_density,
combins[[which.min(MI)]],
format = 2
)
return(
c(
new_clusters[2],
recluster_dependancy(
new_clusters[[1]],
dataset
)
)
)
}
}
joint_density$names <- unlist(names(joint_density$domains))
list(joint_density)
}
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/players.R")
# reclusterer
new_clusters <- recluster_dependancy(joint_density, dataset)
MI
hist(MI)
sort(MI)
clusters <- c(new_clusters, clusters[!seq_along(clusters) %in% groups_to_join])
print(paste0(length(clusters), " clusters, length ", paste0(sapply(clusters, function(x) length(x$names)), collapse = ", ")))
max_cluster_size = Inf
min_cluster_size = 1
joint_distr_size_skip = 500
MI_thresh_for_indep <- 0.05
if (
length(joint_density$domains) == 1 ||
nrow(joint_density$joint_distr) <= joint_distr_size_skip ||
MI_thresh_for_indep == 0
) {
joint_density$names <- names(joint_density$domains)
return(list(joint_density))
}
# faire un gros cluster et pour chq élément regarder si on peut les retirer d'une manière greedy
combins <- generate_partitions(length(joint_density$domains), 2, min_cluster_size = min_cluster_size)
combins_len <- sapply(combins, length)
combins_len
MI <- rep(Inf, length(combins))
len_out
for (j in which(combins_len == len_out)) {
MI[j] <- relative_mutual_information(joint_density, combins[[j]])
}
hist(MI)
head(MI)
min(MI[which(combins_len == len_out)])
MI
min(MI)
combins_len
len_out
combins_len == len_out
which(combins_len == len_out)
MI[which(combins_len == len_out)]
len_out = 2
for (j in which(combins_len == len_out)) {
MI[j] <- relative_mutual_information(joint_density, combins[[j]])
}
min(MI[which(combins_len == len_out)])
min(MI[which(combins_len == len_out)])
min(MI[which(combins_len == len_out - 1)])
min(MI[which(combins_len == len_out - 1)]) < min(MI[which(combins_len == len_out)])
MI <- rep(Inf, length(combins))
# faire toutes les combins qu'on rejette 1 personne, ensuite toutes rejet 2pers, etc.
for (len_out in unique(combins_len)) {
for (j in which(combins_len == len_out)) {
MI[j] <- relative_mutual_information(joint_density, combins[[j]])
}
# si le min de MI de ce groupe est sous le seuil, on essaie d'augmenter la
# taille de rejetés (donc on continue à chercher plus de combins)
# pcq plus on rejette bcp, plus min(MI) sera élevé donc on veut arrêter
# quand tout le groupe dépasse le seuil
if (
min(MI[which(combins_len == len_out)]) >= MI_thresh_for_indep ||
(len_out > 1 && (
min(MI[which(combins_len == len_out - 1)]) < min(MI[which(combins_len == len_out)])
))
){
# print(paste0("MI : ", paste0(round(MI, 3), collapse = ", ")))
# on compare toujours un séparation entre 2 subsets, comme si on a 2 variables
# MI = H(X) + H(Y) - H(X,Y)
# max(H(X), H(Y)) <= H(X)+H(Y)-max(H(X), H(Y))
# borne sup(MI) = H(X) + H(Y) - max(H(x),H(Y))
if (min(MI) < MI_thresh_for_indep) {
print(paste0(sum(MI < Inf), " / ", length(MI)))
# l'exclure du gros cluster in
# TODO moyen d'optimiser car dédouble les calculs déjà faits dans mutual_info()
new_clusters <- marginal_joint_dependancy(joint_density, combins[[which.min(MI)]], format = 2)
return(
c(
new_clusters[2],
recluster_dependancy(new_clusters[[1]], dataset)
)
)
} else {
break
}
}
}
MI <- rep(Inf, length(combins))
# faire toutes les combins qu'on rejette 1 personne, ensuite toutes rejet 2pers, etc.
for (len_out in unique(combins_len)) {
print(len_out)
for (j in which(combins_len == len_out)) {
MI[j] <- relative_mutual_information(joint_density, combins[[j]])
}
# si le min de MI de ce groupe est sous le seuil, on essaie d'augmenter la
# taille de rejetés (donc on continue à chercher plus de combins)
# pcq plus on rejette bcp, plus min(MI) sera élevé donc on veut arrêter
# quand tout le groupe dépasse le seuil
if (
min(MI[which(combins_len == len_out)]) >= MI_thresh_for_indep ||
(len_out > 1 && (
min(MI[which(combins_len == len_out - 1)]) < min(MI[which(combins_len == len_out)])
))
){
# print(paste0("MI : ", paste0(round(MI, 3), collapse = ", ")))
# on compare toujours un séparation entre 2 subsets, comme si on a 2 variables
# MI = H(X) + H(Y) - H(X,Y)
# max(H(X), H(Y)) <= H(X)+H(Y)-max(H(X), H(Y))
# borne sup(MI) = H(X) + H(Y) - max(H(x),H(Y))
if (min(MI) < MI_thresh_for_indep) {
print(paste0(sum(MI < Inf), " / ", length(MI)))
# l'exclure du gros cluster in
# TODO moyen d'optimiser car dédouble les calculs déjà faits dans mutual_info()
new_clusters <- marginal_joint_dependancy(joint_density, combins[[which.min(MI)]], format = 2)
return(
c(
new_clusters[2],
recluster_dependancy(new_clusters[[1]], dataset)
)
)
} else {
break
}
}
}
dataset <- "ping"
if (dataset == "ping") {
include_exact_points <- FALSE
dim_len_mu <- 12
}
if (dataset == "spike") {
include_exact_points <- TRUE
dim_len_mu <- 14
}
if (dataset == "pickle") {
include_exact_points <- TRUE
dim_len_mu <- 18
}
if (dataset == "ping") {
source("src/import_ping.R")
# retirer vieux data
scores <- scores[scores$date >= as.Date("2024-01-01"), ]
#TODO essayer dajouter 10 wins de xav contre vic pour voir cque ca fait
} else if (dataset == "spike") {
source("src/import_spike.R")
} else if (dataset == "pickle") {
source("src/import_pickle.R")
source("src/pickle.R")
}
source("src/update_scores.R")
source("src/plots.R")
source("src/recommend.R")
scores_stats(scores, players)
games_matchups(scores, players)
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
dataset <- "ping"
if (dataset == "ping") {
include_exact_points <- FALSE
dim_len_mu <- 12
}
if (dataset == "spike") {
include_exact_points <- TRUE
dim_len_mu <- 14
}
if (dataset == "pickle") {
include_exact_points <- TRUE
dim_len_mu <- 18
}
if (dataset == "ping") {
source("src/import_ping.R")
# retirer vieux data
scores <- scores[scores$date >= as.Date("2024-01-01"), ]
#TODO essayer dajouter 10 wins de xav contre vic pour voir cque ca fait
} else if (dataset == "spike") {
source("src/import_spike.R")
} else if (dataset == "pickle") {
source("src/import_pickle.R")
source("src/pickle.R")
}
source("src/update_scores.R")
source("src/plots.R")
source("src/recommend.R")
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
init_distr()[, "mu"]
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
init_distr()
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
dataset <- "ping"
if (dataset == "ping") {
include_exact_points <- FALSE
dim_len_mu <- 15
}
if (dataset == "spike") {
include_exact_points <- TRUE
dim_len_mu <- 14
}
if (dataset == "pickle") {
include_exact_points <- TRUE
dim_len_mu <- 18
}
if (dataset == "ping") {
source("src/import_ping.R")
# retirer vieux data
scores <- scores[scores$date >= as.Date("2024-01-01"), ]
#TODO essayer dajouter 10 wins de xav contre vic pour voir cque ca fait
} else if (dataset == "spike") {
source("src/import_spike.R")
} else if (dataset == "pickle") {
source("src/import_pickle.R")
source("src/pickle.R")
}
source("src/update_scores.R")
source("src/plots.R")
source("src/recommend.R")
scores_stats(scores, players)
games_matchups(scores, players)
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
tmp <- show_ranking_history_dependancy(scores, dataset)
source("C:/Users/phbro/Desktop/DOCUMENTS/ActuRank/src/plots.R")
tmp <- show_ranking_history_dependancy(scores, dataset)
dataset <- "ping"
if (dataset == "ping") {
include_exact_points <- FALSE
dim_len_mu <- 15
}
if (dataset == "spike") {
include_exact_points <- TRUE
dim_len_mu <- 14
}
if (dataset == "pickle") {
include_exact_points <- TRUE
dim_len_mu <- 18
}
if (dataset == "ping") {
source("src/import_ping.R")
# retirer vieux data
scores <- scores[scores$date >= as.Date("2024-01-01"), ]
#TODO essayer dajouter 10 wins de xav contre vic pour voir cque ca fait
} else if (dataset == "spike") {
source("src/import_spike.R")
} else if (dataset == "pickle") {
source("src/import_pickle.R")
source("src/pickle.R")
}
source("src/update_scores.R")
source("src/plots.R")
source("src/recommend.R")
scores_stats(scores, players)
games_matchups(scores, players)
set.seed(2024L)
tmp <- show_ranking_history_dependancy(scores, dataset)
47867*100/8
0.00625 * 2^(0:7)
